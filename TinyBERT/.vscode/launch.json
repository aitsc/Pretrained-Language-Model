{
    // 使用 IntelliSense 了解相关属性。 
    // 悬停以查看现有属性的描述。
    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
        },
        // 1. General Distillation
        // python -u pregenerate_training_data.py --train_corpus=data/origin_data/books1.txt --bert_model=data/model/bert-base-uncased --do_lower_case --epochs_to_generate=3 --output_dir=data/gd_data/books1 --num_workers=96
        {
            "name": "GD:生成固定格式语料", // generate the corpus of json format
            "type": "python",
            "request": "launch",
            "program": "pregenerate_training_data.py",
            "console": "integratedTerminal",
            "args": [
                "--train_corpus=data/origin_data/wiki.100.txt",
                "--bert_model=data/model/bert-base-uncased",
                "--do_lower_case", // True
                "--epochs_to_generate=3", // 3
                "--output_dir=data/gd_data/wiki.100", // 对应 --pregenerated_data
                "--num_workers=1",
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "justMyCode": false,
        },
        {
            "name": "GD:内层蒸馏", // run the transformer distillation
            "type": "python",
            "request": "launch",
            "program": "general_distill.py",
            "console": "integratedTerminal",
            "args": [
                "--pregenerated_data=data/gd_data/wiki.100",
                "--teacher_model=data/model/bert-base-uncased",
                "--student_model=data/tinybert",
                "--do_lower_case", // True
                "--train_batch_size=128", // 256
                "--output_dir=", // 输出也模型放在 student_model 下, 之后执行TD .bin要改名去掉迭代
                // "--continue_train", // 是否从检查点开始训练
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "justMyCode": false,
        },
        // 2. Data Augmentation
        // CUDA_VISIBLE_DEVICES=6 python -u data_augmentation.py --pretrained_bert_model=data/model/bert-base-uncased --glove_embs=data/model/glove.42B.300d.txt --glue_dir=data/english_data/GLUE --task_name=RTE
        {
            "name": "数据增强",
            "type": "python",
            "request": "launch",
            "program": "data_augmentation.py",
            "console": "integratedTerminal",
            "args": [
                "--pretrained_bert_model=data/model/bert-base-uncased", // https://huggingface.co/bert-base-uncased/tree/main
                "--glove_embs=data/model/glove.42B.300d.txt", // https://nlp.stanford.edu/projects/glove/
                "--glue_dir=data/english_data/GLUE",
                "--task_name=SST-2", // CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE [WNLI]
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "justMyCode": false,
        },
        // 3. Task-specific Distillation
        // ta -d 6 -o python -u
        {
            "name": "TD:内层蒸馏", // intermediate layer distillation
            "type": "python",
            "request": "launch",
            "program": "task_distill.py",
            "console": "integratedTerminal",
            "args": [
                "--teacher_model=data/model/textattack/bert-base-uncased-",
                "--student_model=data/tinybert/tinybert6",
                "--data_dir=data/english_data/GLUE",
                "--task_name=MRPC", // CoLA SST-2 MRPC STS-B QQP MNLI MNLI-mm QNLI RTE WNLI
                "--output_dir=", // 输出模型放在 student_model/td_inter_{task_name} 下
                "--train_batch_size=32", // 32
                "--aug_train",
                "--do_lower_case", // True
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "justMyCode": false,
        },
        {
            "name": "TD:预测层蒸馏", // prediction layer distillation
            "type": "python",
            "request": "launch",
            "program": "task_distill.py",
            "console": "integratedTerminal",
            "args": [
                "--pred_distill",
                "--teacher_model=data/model/textattack/bert-base-uncased-",
                "--student_model=data/tinybert/tinybert6/td_inter/220801_121244",
                "--data_dir=data/english_data/GLUE",
                "--task_name=MRPC", // CoLA SST-2 MRPC STS-B QQP MNLI MNLI-mm QNLI RTE WNLI
                "--output_dir=", // 输出模型放在 student_model/td_pred 下
                "--aug_train",
                "--do_lower_case", // True
                "--learning_rate=3e-5", // 3e-5
                "--num_train_epochs=3", // 3
                "--eval_step=100", // 100
                "--train_batch_size=32", // 32
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "justMyCode": false,
        },
        // 4. Evaluation
        {
            "name": "评估",
            "type": "python",
            "request": "launch",
            "program": "task_distill.py",
            "console": "integratedTerminal",
            "args": [
                "--do_eval",
                "--student_model=data/tinybert/tinybert6/td_inter/220801_121244/td_pred/220802_015309",
                // "--student_model=data/tinybert/TinyBERT_General_6L_768D",
                // "--student_model=data/model/bert-base-uncased",
                // "--student_model=data/model/bert-large-uncased",
                // "--student_model=data/model/textattack/bert-base-uncased-MNLI",

                "--data_dir=data/english_data/GLUE",
                "--task_name=MRPC", // CoLA SST-2 MRPC STS-B QQP MNLI MNLI-mm QNLI RTE WNLI
                "--output_dir=", // 评估没有输出
                "--do_lower_case", // True
                "--eval_batch_size=32", // 32
                "--max_seq_length=128", // 128
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "justMyCode": false,
        }
    ]
}